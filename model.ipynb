{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranavsingh/joke_rec/joke_recommender_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.transforms import RandomLinkSplit, ToUndirected\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch_geometric.transforms as T\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class JokeGraphBuilder:\n",
    "    def __init__(self, jokes_df):\n",
    "        self.jokes_df = jokes_df\n",
    "        self.edges = []\n",
    "        self.node_features = {}\n",
    "        \n",
    "    def create_content_similarity_edges(self, threshold=0.3, method='sentence_bert'):\n",
    "        \"\"\"Create edges based on content similarity using various methods\"\"\"\n",
    "        \n",
    "        if method == 'sentence_bert':\n",
    "            # Use Sentence-BERT for semantic similarity\n",
    "            model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "            embeddings = model.encode(self.jokes_df['text'].tolist())\n",
    "            similarity_matrix = cosine_similarity(embeddings)\n",
    "            \n",
    "        elif method == 'tfidf':\n",
    "            # Use TF-IDF for lexical similarity\n",
    "            vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "            tfidf_matrix = vectorizer.fit_transform(self.jokes_df['text'])\n",
    "            similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "        \n",
    "        # Create edges for similar jokes\n",
    "        for i in range(len(similarity_matrix)):\n",
    "            for j in range(i+1, len(similarity_matrix)):\n",
    "                if similarity_matrix[i][j] > threshold:\n",
    "                    self.edges.append({\n",
    "                        'source': self.jokes_df.iloc[i]['joke_id'],\n",
    "                        'target': self.jokes_df.iloc[j]['joke_id'],\n",
    "                        'weight': similarity_matrix[i][j],\n",
    "                        'edge_type': f'content_similarity_{method}'\n",
    "                    })\n",
    "\n",
    "    def create_category_edges(self, max_connections_per_joke=15):\n",
    "        \"\"\"Connect jokes within same category (SPARSE VERSION)\"\"\"\n",
    "        import random\n",
    "        \n",
    "        # Same category edges - but limited per joke\n",
    "        category_groups = self.jokes_df.groupby('category')\n",
    "        for category, group in category_groups:\n",
    "            joke_ids = group['joke_id'].tolist()\n",
    "            \n",
    "            # Instead of connecting ALL to ALL, limit connections per joke\n",
    "            for joke_id in joke_ids:\n",
    "                other_jokes = [jid for jid in joke_ids if jid != joke_id]\n",
    "                \n",
    "                # Limit how many jokes each joke connects to\n",
    "                num_connections = min(max_connections_per_joke, len(other_jokes))\n",
    "                if num_connections > 0:\n",
    "                    connected_jokes = random.sample(other_jokes, num_connections)\n",
    "                    \n",
    "                    for connected_joke in connected_jokes:\n",
    "                        self.edges.append({\n",
    "                            'source': joke_id,\n",
    "                            'target': connected_joke,\n",
    "                            'weight': 1.0,\n",
    "                            'edge_type': 'same_category'\n",
    "                        })\n",
    "        \n",
    "        # Related category edges (keep this part as is)\n",
    "        related_categories = {\n",
    "            'Programming': ['programming'],\n",
    "            'Dad Joke': ['Pun'],\n",
    "            'Misc': ['general'],\n",
    "        }\n",
    "        \n",
    "        for main_cat, related_cats in related_categories.items():\n",
    "            main_jokes = self.jokes_df[self.jokes_df['category'] == main_cat]['joke_id'].tolist()\n",
    "            for related_cat in related_cats:\n",
    "                related_jokes = self.jokes_df[self.jokes_df['category'] == related_cat]['joke_id'].tolist()\n",
    "                for main_joke in main_jokes:\n",
    "                    for related_joke in related_jokes:\n",
    "                        self.edges.append({\n",
    "                            'source': main_joke,\n",
    "                            'target': related_joke,\n",
    "                            'weight': 0.7,\n",
    "                            'edge_type': 'related_category'\n",
    "                        })\n",
    "    def create_source_edges(self, weight=0.5):\n",
    "        \"\"\"Connect jokes from the same source\"\"\"\n",
    "        source_groups = self.jokes_df.groupby('source')\n",
    "        for source, group in source_groups:\n",
    "            joke_ids = group['joke_id'].tolist()\n",
    "            # Create edges between jokes from same source\n",
    "            for i in range(len(joke_ids)):\n",
    "                for j in range(i+1, min(i+6, len(joke_ids))):  # Limit to avoid too many edges\n",
    "                    self.edges.append({\n",
    "                        'source': joke_ids[i],\n",
    "                        'target': joke_ids[j],\n",
    "                        'weight': weight,\n",
    "                        'edge_type': 'same_source'\n",
    "                    })\n",
    "    \n",
    "    def create_keyword_entity_edges(self):\n",
    "        \"\"\"Connect jokes that share keywords or named entities\"\"\"\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        \n",
    "        joke_keywords = {}\n",
    "        joke_entities = {}\n",
    "        \n",
    "        for idx, row in self.jokes_df.iterrows():\n",
    "            doc = nlp(row['text'])\n",
    "            \n",
    "            # Extract keywords (nouns, verbs, adjectives)\n",
    "            keywords = [token.lemma_.lower() for token in doc \n",
    "                       if token.pos_ in ['NOUN', 'VERB', 'ADJ'] and len(token.text) > 3]\n",
    "            joke_keywords[row['joke_id']] = set(keywords)\n",
    "            \n",
    "            # Extract named entities\n",
    "            entities = [ent.text.lower() for ent in doc.ents]\n",
    "            joke_entities[row['joke_id']] = set(entities)\n",
    "        \n",
    "        # Create edges based on shared keywords\n",
    "        joke_ids = list(joke_keywords.keys())\n",
    "        for i in range(len(joke_ids)):\n",
    "            for j in range(i+1, len(joke_ids)):\n",
    "                id1, id2 = joke_ids[i], joke_ids[j]\n",
    "                \n",
    "                # Keyword overlap\n",
    "                keyword_overlap = len(joke_keywords[id1] & joke_keywords[id2])\n",
    "                if keyword_overlap >= 2:  # At least 2 shared keywords\n",
    "                    self.edges.append({\n",
    "                        'source': id1,\n",
    "                        'target': id2,\n",
    "                        'weight': min(keyword_overlap * 0.2, 1.0),\n",
    "                        'edge_type': 'shared_keywords'\n",
    "                    })\n",
    "                \n",
    "                # Entity overlap\n",
    "                entity_overlap = len(joke_entities[id1] & joke_entities[id2])\n",
    "                if entity_overlap >= 1:  # At least 1 shared entity\n",
    "                    self.edges.append({\n",
    "                        'source': id1,\n",
    "                        'target': id2,\n",
    "                        'weight': entity_overlap * 0.3,\n",
    "                        'edge_type': 'shared_entities'\n",
    "                    })\n",
    "    \n",
    "    def create_structural_similarity_edges(self):\n",
    "        \"\"\"Connect jokes with similar structure (length, punctuation, etc.)\"\"\"\n",
    "        \n",
    "        # Calculate structural features\n",
    "        structural_features = []\n",
    "        for text in self.jokes_df['text']:\n",
    "            features = {\n",
    "                'length': len(text),\n",
    "                'word_count': len(text.split()),\n",
    "                'question_marks': text.count('?'),\n",
    "                'exclamation_marks': text.count('!'),\n",
    "                'periods': text.count('.'),\n",
    "                'has_dialogue': '\"' in text or \"'\" in text,\n",
    "                'has_numbers': bool(re.search(r'\\d', text))\n",
    "            }\n",
    "            structural_features.append(features)\n",
    "        \n",
    "        # Create edges for structurally similar jokes\n",
    "        for i in range(len(structural_features)):\n",
    "            for j in range(i+1, len(structural_features)):\n",
    "                f1, f2 = structural_features[i], structural_features[j]\n",
    "                \n",
    "                # Calculate structural similarity\n",
    "                similarity = 0\n",
    "                \n",
    "                # Length similarity\n",
    "                length_diff = abs(f1['length'] - f2['length'])\n",
    "                if length_diff < 50:  # Similar length\n",
    "                    similarity += 0.3\n",
    "                \n",
    "                # Similar punctuation patterns\n",
    "                if f1['question_marks'] > 0 and f2['question_marks'] > 0:\n",
    "                    similarity += 0.2\n",
    "                if f1['exclamation_marks'] > 0 and f2['exclamation_marks'] > 0:\n",
    "                    similarity += 0.2\n",
    "                if f1['has_dialogue'] == f2['has_dialogue']:\n",
    "                    similarity += 0.2\n",
    "                if f1['has_numbers'] == f2['has_numbers']:\n",
    "                    similarity += 0.1\n",
    "                \n",
    "                if similarity > 0.4:  # Threshold for structural similarity\n",
    "                    self.edges.append({\n",
    "                        'source': self.jokes_df.iloc[i]['joke_id'],\n",
    "                        'target': self.jokes_df.iloc[j]['joke_id'],\n",
    "                        'weight': similarity,\n",
    "                        'edge_type': 'structural_similarity'\n",
    "                    })\n",
    "    \n",
    "    \n",
    "    def create_topic_modeling_edges(self, n_topics=10, threshold=0.3):  # Add threshold param\n",
    "        \"\"\"Connect jokes that share topics discovered through topic modeling\"\"\"\n",
    "        from sklearn.decomposition import LatentDirichletAllocation\n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "        \n",
    "        # Prepare text data\n",
    "        vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n",
    "        doc_term_matrix = vectorizer.fit_transform(self.jokes_df['text'])\n",
    "        \n",
    "        # Fit LDA model\n",
    "        lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "        topic_distributions = lda.fit_transform(doc_term_matrix)\n",
    "        \n",
    "        # Create edges between jokes with similar topic distributions\n",
    "        similarity_matrix = cosine_similarity(topic_distributions)\n",
    "        \n",
    "        for i in range(len(similarity_matrix)):\n",
    "            for j in range(i+1, len(similarity_matrix)):\n",
    "                if similarity_matrix[i][j] > threshold:  # Use parameter instead of hardcoded 0.3\n",
    "                    self.edges.append({\n",
    "                        'source': self.jokes_df.iloc[i]['joke_id'],\n",
    "                        'target': self.jokes_df.iloc[j]['joke_id'],\n",
    "                        'weight': similarity_matrix[i][j],\n",
    "                        'edge_type': 'topic_similarity'\n",
    "                    })\n",
    "    \n",
    "    def create_difficulty_complexity_edges(self):\n",
    "        \"\"\"Connect jokes with similar complexity/difficulty levels\"\"\"\n",
    "        \n",
    "        complexity_scores = []\n",
    "        for text in self.jokes_df['text']:\n",
    "            # Simple complexity metrics\n",
    "            avg_word_length = np.mean([len(word) for word in text.split()])\n",
    "            sentence_count = len([s for s in text.split('.') if s.strip()])\n",
    "            vocabulary_richness = len(set(text.lower().split())) / len(text.split())\n",
    "            \n",
    "            complexity = (avg_word_length * 0.4 + \n",
    "                         sentence_count * 0.3 + \n",
    "                         vocabulary_richness * 0.3)\n",
    "            complexity_scores.append(complexity)\n",
    "        \n",
    "        # Group jokes by complexity level\n",
    "        complexity_quartiles = np.percentile(complexity_scores, [25, 50, 75])\n",
    "        \n",
    "        for i in range(len(complexity_scores)):\n",
    "            for j in range(i+1, len(complexity_scores)):\n",
    "                # Connect jokes in same complexity quartile\n",
    "                score1, score2 = complexity_scores[i], complexity_scores[j]\n",
    "                \n",
    "                if abs(score1 - score2) < 0.5:  # Similar complexity\n",
    "                    self.edges.append({\n",
    "                        'source': self.jokes_df.iloc[i]['joke_id'],\n",
    "                        'target': self.jokes_df.iloc[j]['joke_id'],\n",
    "                        'weight': 1.0 - abs(score1 - score2),\n",
    "                        'edge_type': 'complexity_similarity'\n",
    "                    })\n",
    "    \n",
    "    def generate_node_features(self):\n",
    "        \"\"\"Generate comprehensive node features\"\"\"\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        embeddings = model.encode(self.jokes_df['text'].tolist())\n",
    "        \n",
    "        for idx, row in self.jokes_df.iterrows():\n",
    "            text = row['text']\n",
    "            \n",
    "            # Basic features\n",
    "            features = {\n",
    "                'joke_id': row['joke_id'],\n",
    "                'category': row['category'],\n",
    "                'source': row['source'],\n",
    "                'text_length': len(text),\n",
    "                'word_count': len(text.split()),\n",
    "                'sentence_count': len([s for s in text.split('.') if s.strip()]),\n",
    "                'question_count': text.count('?'),\n",
    "                'exclamation_count': text.count('!'),\n",
    "                'has_dialogue': '\"' in text or \"'\" in text,\n",
    "                'has_numbers': bool(re.search(r'\\d', text)),\n",
    "                'embedding': embeddings[idx].tolist()  # Sentence-BERT embedding\n",
    "            }\n",
    "            \n",
    "            self.node_features[row['joke_id']] = features\n",
    "    \n",
    "    def build_complete_graph(self):\n",
    "        \"\"\"Build the complete joke recommendation graph\"\"\"\n",
    "        print(\"Generating node features...\")\n",
    "        self.generate_node_features()\n",
    "        \n",
    "        print(\"Creating content similarity edges...\")\n",
    "        self.create_content_similarity_edges(threshold=0.4)\n",
    "        \n",
    "        print(\"Creating category edges...\")\n",
    "        self.create_category_edges(max_connections_per_joke=15)\n",
    "        \n",
    "        print(\"Creating source edges...\")\n",
    "        self.create_source_edges()\n",
    "        \n",
    "        print(\"Creating keyword/entity edges...\")\n",
    "        self.create_keyword_entity_edges()\n",
    "        \n",
    "        \n",
    "        print(\"Creating topic modeling edges...\")\n",
    "        self.create_topic_modeling_edges(threshold = 0.6 ,n_topics=7)\n",
    "        \n",
    "        # print(\"Creating complexity edges...\")\n",
    "        # self.create_difficulty_complexity_edges()\n",
    "        \n",
    "        print(f\"Graph built with {len(self.node_features)} nodes and {len(self.edges)} edges\")\n",
    "        \n",
    "        return self.node_features, self.edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example:\n",
    "jokes_df = pd.read_csv('jokes_dataset_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BALANCING DATASET:\n",
      "==============================\n",
      "Programming: 39 jokes (was 39)\n",
      "Dark: 10 jokes (was 10)\n",
      "Pun: 11 jokes (was 11)\n",
      "Misc: 13 jokes (was 13)\n",
      "Dad Joke: 40 jokes (was 552)\n",
      "programming: 27 jokes (was 27)\n",
      "general: 40 jokes (was 65)\n",
      "\n",
      "Total: 180 jokes (was 717)\n",
      "\n",
      "CLEANING CATEGORIES:\n",
      "====================\n",
      "Category counts after cleaning:\n",
      "category\n",
      "Programming    66\n",
      "Misc           53\n",
      "Dad Joke       40\n",
      "Pun            11\n",
      "Dark           10\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def balance_dataset(jokes_df, max_per_category=40):\n",
    "    \"\"\"Create a balanced subset for better learning\"\"\"\n",
    "    \n",
    "    print(\"BALANCING DATASET:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    balanced_jokes = []\n",
    "    \n",
    "    for category in jokes_df['category'].unique():\n",
    "        cat_jokes = jokes_df[jokes_df['category'] == category]\n",
    "        \n",
    "        # Take at most max_per_category jokes per category\n",
    "        if len(cat_jokes) > max_per_category:\n",
    "            cat_jokes = cat_jokes.sample(max_per_category, random_state=42)\n",
    "        \n",
    "        balanced_jokes.append(cat_jokes)\n",
    "        print(f\"{category}: {len(cat_jokes)} jokes (was {len(jokes_df[jokes_df['category'] == category])})\")\n",
    "    \n",
    "    balanced_df = pd.concat(balanced_jokes, ignore_index=True)\n",
    "    print(f\"\\nTotal: {len(balanced_df)} jokes (was {len(jokes_df)})\")\n",
    "    \n",
    "    return balanced_df\n",
    "\n",
    "def clean_categories(jokes_df):\n",
    "    \"\"\"Merge similar categories\"\"\"\n",
    "    \n",
    "    print(\"\\nCLEANING CATEGORIES:\")\n",
    "    print(\"=\" * 20)\n",
    "    \n",
    "    # Fix the obvious duplicates\n",
    "    jokes_df = jokes_df.copy()\n",
    "    jokes_df['category'] = jokes_df['category'].replace({\n",
    "        'programming': 'Programming',  # Merge case variants\n",
    "        'general': 'Misc'              # Merge similar categories\n",
    "    })\n",
    "    \n",
    "    print(\"Category counts after cleaning:\")\n",
    "    print(jokes_df['category'].value_counts())\n",
    "    \n",
    "    return jokes_df\n",
    "\n",
    "# Apply the fixes\n",
    "balanced_df = balance_dataset(jokes_df, max_per_category=40)\n",
    "balanced_df = clean_categories(balanced_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating node features...\n",
      "Creating content similarity edges...\n",
      "Creating category edges...\n",
      "Creating source edges...\n",
      "Creating keyword/entity edges...\n",
      "Creating topic modeling edges...\n",
      "Graph built with 180 nodes and 6455 edges\n"
     ]
    }
   ],
   "source": [
    "graph_builder = JokeGraphBuilder(balanced_df)\n",
    "node_features, edges = graph_builder.build_complete_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_pytorch_geometric(node_features, edges):\n",
    "    \"\"\"Convert with extensive debugging\"\"\"\n",
    "    \n",
    "    # Create node mapping\n",
    "    node_ids = list(node_features.keys())\n",
    "    node_to_idx = {node_id: idx for idx, node_id in enumerate(node_ids)}\n",
    "    print(f\"Number of nodes: {len(node_ids)}\")\n",
    "    print(f\"Sample node IDs: {node_ids[:5]}\")\n",
    "    \n",
    "    # Extract node features (combine embedding + other features)\n",
    "    x = []\n",
    "    for node_id in node_ids:\n",
    "        features = node_features[node_id]\n",
    "        \n",
    "        # Start with embedding\n",
    "        feature_vector = features['embedding'].copy()\n",
    "        \n",
    "        # Add other numerical features\n",
    "        feature_vector.extend([\n",
    "            features.get('length', 0),\n",
    "            features.get('word_count', 0),\n",
    "            features.get('sentence_count', 0),\n",
    "            features.get('question_count', 0),\n",
    "            features.get('exclamation_count', 0),\n",
    "            float(features.get('has_dialogue', 0)),\n",
    "        ])\n",
    "        \n",
    "        x.append(feature_vector)\n",
    "    \n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "    print(f\"Node features tensor shape: {x.shape}\")\n",
    "    \n",
    "    # Group edges by type\n",
    "    edge_types_dict = {}\n",
    "    for edge in edges:\n",
    "        edge_type = edge['edge_type']\n",
    "        if edge_type not in edge_types_dict:\n",
    "            edge_types_dict[edge_type] = {'source': [], 'target': [], 'weight': []}\n",
    "        \n",
    "        # Check if nodes exist in mapping\n",
    "        if edge['source'] not in node_to_idx:\n",
    "            print(f\"Skipping edge: source node {edge['source']} not found\")\n",
    "            continue\n",
    "        if edge['target'] not in node_to_idx:\n",
    "            print(f\"Skipping edge: target node {edge['target']} not found\")\n",
    "            continue\n",
    "            \n",
    "        src_idx = node_to_idx[edge['source']]\n",
    "        tgt_idx = node_to_idx[edge['target']]\n",
    "        \n",
    "        # Add both directions for undirected graph\n",
    "        edge_types_dict[edge_type]['source'].extend([src_idx, tgt_idx])\n",
    "        edge_types_dict[edge_type]['target'].extend([tgt_idx, src_idx])\n",
    "        edge_types_dict[edge_type]['weight'].extend([edge['weight'], edge['weight']])\n",
    "    \n",
    "    print(f\"Edge types processed: {list(edge_types_dict.keys())}\")\n",
    "    for edge_type, edge_data in edge_types_dict.items():\n",
    "        print(f\"  {edge_type}: {len(edge_data['source'])} directed edges\")\n",
    "    \n",
    "    # Create HeteroData\n",
    "    data = HeteroData()\n",
    "    data['joke'].x = x\n",
    "    \n",
    "    for edge_type, edge_data in edge_types_dict.items():\n",
    "        if len(edge_data['source']) == 0:\n",
    "            print(f\"⚠️ No valid edges for type: {edge_type}\")\n",
    "            continue\n",
    "            \n",
    "        edge_index = torch.tensor([edge_data['source'], edge_data['target']], dtype=torch.long)\n",
    "        edge_attr = torch.tensor(edge_data['weight'], dtype=torch.float)\n",
    "        \n",
    "        print(f\"Creating edge type '{edge_type}': edge_index shape {edge_index.shape}\")\n",
    "        \n",
    "        data['joke', edge_type, 'joke'].edge_index = edge_index\n",
    "        data['joke', edge_type, 'joke'].edge_attr = edge_attr\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONVERSION DEBUG ===\n",
      "Number of nodes: 180\n",
      "Sample node IDs: ['23', '54', '36', '40', '28']\n",
      "Node features tensor shape: torch.Size([180, 390])\n",
      "Edge types processed: ['content_similarity_sentence_bert', 'same_category', 'related_category', 'same_source', 'shared_keywords', 'shared_entities', 'topic_similarity']\n",
      "  content_similarity_sentence_bert: 208 directed edges\n",
      "  same_category: 5170 directed edges\n",
      "  related_category: 880 directed edges\n",
      "  same_source: 1710 directed edges\n",
      "  shared_keywords: 266 directed edges\n",
      "  shared_entities: 182 directed edges\n",
      "  topic_similarity: 4494 directed edges\n",
      "Creating edge type 'content_similarity_sentence_bert': edge_index shape torch.Size([2, 208])\n",
      "Creating edge type 'same_category': edge_index shape torch.Size([2, 5170])\n",
      "Creating edge type 'related_category': edge_index shape torch.Size([2, 880])\n",
      "Creating edge type 'same_source': edge_index shape torch.Size([2, 1710])\n",
      "Creating edge type 'shared_keywords': edge_index shape torch.Size([2, 266])\n",
      "Creating edge type 'shared_entities': edge_index shape torch.Size([2, 182])\n",
      "Creating edge type 'topic_similarity': edge_index shape torch.Size([2, 4494])\n"
     ]
    }
   ],
   "source": [
    "data = convert_to_pytorch_geometric(node_features, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import HGTConv, Linear\n",
    "import torch.nn as nn\n",
    "\n",
    "class JokeRecommendationHGT(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_channels=128, embedding_dim=64, \n",
    "                 num_heads=4, num_layers=2, data=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Node type projections (just 'joke' in your case)\n",
    "        self.lin_dict = torch.nn.ModuleDict()\n",
    "        for node_type in data.node_types:\n",
    "            self.lin_dict[node_type] = Linear(input_dim, hidden_channels)\n",
    "        \n",
    "        # HGT layers to process different edge types\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv = HGTConv(\n",
    "                hidden_channels, \n",
    "                hidden_channels, \n",
    "                data.metadata(),\n",
    "                num_heads, \n",
    "                # group='sum'\n",
    "            )\n",
    "            self.convs.append(conv)\n",
    "        \n",
    "        # Final embedding projection\n",
    "        self.embedding_proj = Linear(hidden_channels, embedding_dim)\n",
    "        \n",
    "        # Optional: Add a classifier head for categories\n",
    "        self.category_classifier = nn.Sequential(\n",
    "            Linear(embedding_dim, hidden_channels // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            Linear(hidden_channels // 2, 7)  # 7 categories in your data\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_dict, edge_index_dict, return_embeddings=True):\n",
    "        # Initial transformation\n",
    "        for node_type, x in x_dict.items():\n",
    "            x_dict[node_type] = F.relu(self.lin_dict[node_type](x))\n",
    "        \n",
    "        # Apply HGT convolutions\n",
    "        for conv in self.convs:\n",
    "            x_dict = conv(x_dict, edge_index_dict)\n",
    "        \n",
    "        # Get final joke embeddings\n",
    "        joke_embeddings = self.embedding_proj(x_dict['joke'])\n",
    "        \n",
    "        if return_embeddings:\n",
    "            return joke_embeddings\n",
    "        else:\n",
    "            # Return category predictions\n",
    "            return self.category_classifier(joke_embeddings)\n",
    "    \n",
    "    def get_joke_similarities(self, x_dict, edge_index_dict, joke_indices=None):\n",
    "        \"\"\"Get similarity scores between jokes\"\"\"\n",
    "        embeddings = self.forward(x_dict, edge_index_dict, return_embeddings=True)\n",
    "        \n",
    "        if joke_indices is not None:\n",
    "            embeddings = embeddings[joke_indices]\n",
    "        \n",
    "        # Compute pairwise similarities\n",
    "        similarities = torch.mm(embeddings, embeddings.t())\n",
    "        return similarities\n",
    "    \n",
    "    def recommend_jokes(self, x_dict, edge_index_dict, source_joke_idx, top_k=5):\n",
    "        \"\"\"Recommend top-k similar jokes to a source joke\"\"\"\n",
    "        embeddings = self.forward(x_dict, edge_index_dict, return_embeddings=True)\n",
    "        \n",
    "        # Get source joke embedding\n",
    "        source_embedding = embeddings[source_joke_idx].unsqueeze(0)\n",
    "        \n",
    "        # Compute similarities with all jokes\n",
    "        similarities = torch.mm(source_embedding, embeddings.t()).squeeze()\n",
    "        \n",
    "        # Get top-k most similar (excluding the source joke itself)\n",
    "        similarities[source_joke_idx] = -float('inf')  # Exclude self\n",
    "        top_k_indices = similarities.topk(top_k).indices\n",
    "        top_k_scores = similarities.topk(top_k).values\n",
    "        \n",
    "        return top_k_indices, top_k_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_split(data, train_ratio=0.7, val_ratio=0.15):\n",
    "    \"\"\"Dead simple splitting - no fancy PyG needed\"\"\"\n",
    "    \n",
    "    # Just use your full graph for all splits (small graph = no memory issues)\n",
    "    train_data = data\n",
    "    val_data = data  \n",
    "    test_data = data\n",
    "    \n",
    "    # Create random masks for evaluation\n",
    "    num_nodes = data['joke'].x.shape[0]\n",
    "    indices = torch.randperm(num_nodes)\n",
    "    \n",
    "    train_end = int(train_ratio * num_nodes)\n",
    "    val_end = int((train_ratio + val_ratio) * num_nodes)\n",
    "    \n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    \n",
    "    train_mask[indices[:train_end]] = True\n",
    "    val_mask[indices[train_end:val_end]] = True\n",
    "    test_mask[indices[val_end:]] = True\n",
    "    \n",
    "    return train_data, val_data, test_data, train_mask, val_mask, test_mask\n",
    "\n",
    "# 2. Simple training loop\n",
    "def simple_train(model, data, train_mask, val_mask, num_epochs=100):\n",
    "    \"\"\"Simple full-batch training - no loaders needed\"\"\"\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass on ENTIRE graph (it's small!)\n",
    "        embeddings = model(data.x_dict, data.edge_index_dict)\n",
    "        \n",
    "        # Simple contrastive loss on training nodes\n",
    "        train_embeddings = embeddings[train_mask]\n",
    "        loss = simple_contrastive_loss(train_embeddings)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_embeddings = embeddings[val_mask]\n",
    "                val_loss = simple_contrastive_loss(val_embeddings)\n",
    "            \n",
    "            print(f\"Epoch {epoch}: Train={loss:.4f}, Val={val_loss:.4f}\")\n",
    "\n",
    "def simple_contrastive_loss(embeddings):\n",
    "    \"\"\"Super simple loss - just make embeddings diverse\"\"\"\n",
    "    # Random positive and negative pairs\n",
    "    n = embeddings.shape[0]\n",
    "    if n < 2:\n",
    "        return torch.tensor(0.0)\n",
    "    \n",
    "    # Sample some pairs\n",
    "    num_pairs = min(100, n//2)\n",
    "    idx1 = torch.randint(0, n, (num_pairs,))\n",
    "    idx2 = torch.randint(0, n, (num_pairs,))\n",
    "    \n",
    "    # Simple distance loss\n",
    "    distances = F.pairwise_distance(embeddings[idx1], embeddings[idx2])\n",
    "    return distances.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_proper_evaluation(data):\n",
    "    \"\"\"Create proper train/val/test edges for recommendation evaluation\"\"\"\n",
    "    \n",
    "    # Get actual edges from your graph (jokes that are connected)\n",
    "    main_edge_type = list(data.edge_types)[0]  # Use your main edge type\n",
    "    edge_index = data[main_edge_type].edge_index\n",
    "    \n",
    "    # Convert to edge list and remove duplicates\n",
    "    edges = edge_index.t().numpy()\n",
    "    unique_edges = []\n",
    "    seen = set()\n",
    "    \n",
    "    for edge in edges:\n",
    "        edge_tuple = tuple(sorted([edge[0], edge[1]]))\n",
    "        if edge_tuple not in seen and edge[0] != edge[1]:\n",
    "            seen.add(edge_tuple)\n",
    "            unique_edges.append([edge[0], edge[1]])\n",
    "    \n",
    "    unique_edges = np.array(unique_edges)\n",
    "    print(f\"Total unique edges: {len(unique_edges)}\")\n",
    "    \n",
    "    # Split edges into train/val/test\n",
    "    train_size = int(0.7 * len(unique_edges))\n",
    "    val_size = int(0.15 * len(unique_edges))\n",
    "    \n",
    "    train_edges = unique_edges[:train_size]\n",
    "    val_edges = unique_edges[train_size:train_size + val_size]\n",
    "    test_edges = unique_edges[train_size + val_size:]\n",
    "    \n",
    "    print(f\"Train edges: {len(train_edges)}, Val edges: {len(val_edges)}, Test edges: {len(test_edges)}\")\n",
    "    \n",
    "    return train_edges, val_edges, test_edges\n",
    "\n",
    "def create_negative_edges(positive_edges, num_nodes, num_negatives):\n",
    "    \"\"\"Create negative edge samples\"\"\"\n",
    "    existing_edges = set()\n",
    "    for edge in positive_edges:\n",
    "        existing_edges.add((edge[0], edge[1]))\n",
    "        existing_edges.add((edge[1], edge[0]))\n",
    "    \n",
    "    negative_edges = []\n",
    "    while len(negative_edges) < num_negatives:\n",
    "        src = np.random.randint(0, num_nodes)\n",
    "        dst = np.random.randint(0, num_nodes)\n",
    "        \n",
    "        if src != dst and (src, dst) not in existing_edges:\n",
    "            negative_edges.append([src, dst])\n",
    "            existing_edges.add((src, dst))\n",
    "            existing_edges.add((dst, src))\n",
    "    \n",
    "    return np.array(negative_edges)\n",
    "\n",
    "def evaluate_link_prediction(model, data, test_edges, test_negatives):\n",
    "    \"\"\"Properly evaluate link prediction performance\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get embeddings\n",
    "        embeddings = model(data.x_dict, data.edge_index_dict)\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        # Positive predictions\n",
    "        pos_scores = []\n",
    "        for edge in test_edges:\n",
    "            src_emb = embeddings[edge[0]]\n",
    "            dst_emb = embeddings[edge[1]]\n",
    "            score = torch.dot(src_emb, dst_emb).item()\n",
    "            pos_scores.append(score)\n",
    "        \n",
    "        # Negative predictions  \n",
    "        neg_scores = []\n",
    "        for edge in test_negatives:\n",
    "            src_emb = embeddings[edge[0]]\n",
    "            dst_emb = embeddings[edge[1]]\n",
    "            score = torch.dot(src_emb, dst_emb).item()\n",
    "            neg_scores.append(score)\n",
    "        \n",
    "        # Combine for evaluation\n",
    "        all_scores = pos_scores + neg_scores\n",
    "        all_labels = [1] * len(pos_scores) + [0] * len(neg_scores)\n",
    "        \n",
    "        # Calculate AUC\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        auc = roc_auc_score(all_labels, all_scores)\n",
    "        \n",
    "        # Calculate accuracy with threshold 0\n",
    "        pred_labels = [1 if score > 0 else 0 for score in all_scores]\n",
    "        accuracy = sum([p == l for p, l in zip(pred_labels, all_labels)]) / len(all_labels)\n",
    "        \n",
    "        return {\n",
    "            'auc': auc,\n",
    "            'accuracy': accuracy,\n",
    "            'pos_score_mean': np.mean(pos_scores),\n",
    "            'neg_score_mean': np.mean(neg_scores)\n",
    "        }\n",
    "\n",
    "def proper_train_with_link_prediction(model, data, train_edges, val_edges, num_epochs=100):\n",
    "    \"\"\"Train with proper link prediction objective\"\"\"\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    num_nodes = data['joke'].x.shape[0]\n",
    "    \n",
    "    # Create validation negatives once\n",
    "    val_negatives = create_negative_edges(val_edges, num_nodes, len(val_edges))\n",
    "    \n",
    "    best_val_auc = 0\n",
    "    patience = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        # Sample training batch\n",
    "        batch_size = min(256, len(train_edges))\n",
    "        batch_indices = np.random.choice(len(train_edges), batch_size, replace=False)\n",
    "        batch_edges = train_edges[batch_indices]\n",
    "        \n",
    "        # Create negative edges for this batch\n",
    "        batch_negatives = create_negative_edges(batch_edges, num_nodes, len(batch_edges))\n",
    "        \n",
    "        # Combine positive and negative edges\n",
    "        all_edges = np.vstack([batch_edges, batch_negatives])\n",
    "        labels = torch.cat([\n",
    "            torch.ones(len(batch_edges)),\n",
    "            torch.zeros(len(batch_negatives))\n",
    "        ]).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        embeddings = model(data.x_dict, data.edge_index_dict)\n",
    "        \n",
    "        # Compute edge predictions\n",
    "        edge_scores = []\n",
    "        for edge in all_edges:\n",
    "            src_emb = embeddings[edge[0]]\n",
    "            dst_emb = embeddings[edge[1]]\n",
    "            score = torch.dot(src_emb, dst_emb)\n",
    "            edge_scores.append(score)\n",
    "        \n",
    "        edge_scores = torch.stack(edge_scores)\n",
    "        \n",
    "        # Binary cross-entropy loss (this is what you suggested!)\n",
    "        loss = F.binary_cross_entropy_with_logits(edge_scores, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            val_metrics = evaluate_link_prediction(model, data, val_edges, val_negatives)\n",
    "            \n",
    "            print(f\"Epoch {epoch:3d} | \"\n",
    "                  f\"Loss: {loss:.4f} | \"\n",
    "                  f\"Val AUC: {val_metrics['auc']:.4f} | \"\n",
    "                  f\"Val Acc: {val_metrics['accuracy']:.4f} | \"\n",
    "                  f\"Pos: {val_metrics['pos_score_mean']:.3f} | \"\n",
    "                  f\"Neg: {val_metrics['neg_score_mean']:.3f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_metrics['auc'] > best_val_auc:\n",
    "                best_val_auc = val_metrics['auc']\n",
    "                patience = 0\n",
    "            else:\n",
    "                patience += 1\n",
    "                \n",
    "            if patience >= 10:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DATA QUALITY LOOKS GOOD - PROCEEDING WITH TRAINING\n",
      "============================================================\n",
      "Total unique edges: 104\n",
      "Train edges: 72, Val edges: 15, Test edges: 17\n",
      "Training edges: 72\n",
      "Validation edges: 15\n",
      "Test edges: 17\n",
      "\n",
      "🔥 TRAINING WITH IMPROVED DATA:\n",
      "Epoch   0 | Loss: 0.6959 | Val AUC: 0.7822 | Val Acc: 0.5000 | Pos: 0.989 | Neg: 0.944\n",
      "Epoch  10 | Loss: 0.5916 | Val AUC: 0.6889 | Val Acc: 0.7333 | Pos: 0.847 | Neg: 0.245\n",
      "Epoch  20 | Loss: 0.4579 | Val AUC: 0.6978 | Val Acc: 0.7000 | Pos: 0.797 | Neg: 0.206\n",
      "Epoch  30 | Loss: 0.4527 | Val AUC: 0.6578 | Val Acc: 0.6667 | Pos: 0.722 | Neg: 0.059\n",
      "Epoch  40 | Loss: 0.4282 | Val AUC: 0.6089 | Val Acc: 0.6333 | Pos: 0.671 | Neg: 0.165\n",
      "Epoch  50 | Loss: 0.4466 | Val AUC: 0.6800 | Val Acc: 0.6000 | Pos: 0.693 | Neg: 0.241\n",
      "Epoch  60 | Loss: 0.4042 | Val AUC: 0.6133 | Val Acc: 0.6000 | Pos: 0.431 | Neg: 0.200\n",
      "Epoch  70 | Loss: 0.4420 | Val AUC: 0.7289 | Val Acc: 0.6000 | Pos: 0.707 | Neg: 0.301\n",
      "Epoch  80 | Loss: 0.4551 | Val AUC: 0.6978 | Val Acc: 0.6000 | Pos: 0.725 | Neg: 0.327\n",
      "Epoch  90 | Loss: 0.5654 | Val AUC: 0.5689 | Val Acc: 0.6000 | Pos: 0.717 | Neg: 0.357\n",
      "AUC: 0.8270\n",
      "Accuracy: 0.7647\n",
      "Positive scores (mean): 0.861\n",
      "Negative scores (mean): -0.065\n"
     ]
    }
   ],
   "source": [
    "train_edges, val_edges, test_edges = create_proper_evaluation(data)\n",
    "\n",
    "print(f\"Training edges: {len(train_edges)}\") \n",
    "print(f\"Validation edges: {len(val_edges)}\")\n",
    "print(f\"Test edges: {len(test_edges)}\")\n",
    "\n",
    "# Initialize model\n",
    "model = JokeRecommendationHGT(\n",
    "    input_dim=data['joke'].x.shape[1],\n",
    "    hidden_channels=128,\n",
    "    embedding_dim=64,\n",
    "    num_heads=4,\n",
    "    num_layers=2,\n",
    "    data=data\n",
    ")\n",
    "\n",
    "# Train with the improved data\n",
    "model = proper_train_with_link_prediction(model, data, train_edges, val_edges)\n",
    "\n",
    "# Final evaluation\n",
    "num_nodes = data['joke'].x.shape[0]\n",
    "test_negatives = create_negative_edges(test_edges, num_nodes, len(test_edges))\n",
    "\n",
    "print(f\"AUC: {test_metrics['auc']:.4f}\")\n",
    "print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")  \n",
    "print(f\"Positive scores (mean): {test_metrics['pos_score_mean']:.3f}\")\n",
    "print(f\"Negative scores (mean): {test_metrics['neg_score_mean']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_recommendation_quality(model, data, balanced_df):\n",
    "    \"\"\"Test if recommendations are actually meaningful\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(data.x_dict, data.edge_index_dict)\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    \n",
    "        # Test different categories\n",
    "        test_categories = ['Programming', 'Dad Joke', 'Misc']\n",
    "        \n",
    "        for category in test_categories:\n",
    "            cat_jokes = balanced_df[balanced_df['category'] == category]\n",
    "            if len(cat_jokes) > 0:\n",
    "                # Pick random joke from category\n",
    "                test_joke = cat_jokes.sample(1).iloc[0]\n",
    "                joke_idx = balanced_df[balanced_df['joke_id'] == test_joke['joke_id']].index[0]\n",
    "                \n",
    "                # Get recommendations\n",
    "                similarities = torch.mm(embeddings[joke_idx:joke_idx+1], embeddings.t()).squeeze()\n",
    "                top_5_indices = similarities.topk(6)[1][1:]  # Skip self\n",
    "                \n",
    "                print(f\"source ({category}):\")\n",
    "                print(f\"   {test_joke['text'][:100]}...\")\n",
    "                \n",
    "                print(f\"top 5 recommendations:\")\n",
    "                for i, idx in enumerate(top_5_indices):\n",
    "                    rec_joke = balanced_df.iloc[idx.item()]\n",
    "                    score = similarities[idx].item()\n",
    "                    print(f\"   {i+1}. ({rec_joke['category']}) [{score:.3f}] {rec_joke['text'][:80]}...\")\n",
    "                \n",
    "                # Check category consistency\n",
    "                rec_categories = [balanced_df.iloc[idx.item()]['category'] for idx in top_5_indices]\n",
    "                same_category_count = sum([1 for cat in rec_categories if cat == category])\n",
    "                print(f\"{same_category_count}/5 recommendations from same category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source (Programming):\n",
      "   Why did the programmer's wife leave him? He didn't know how to commit....\n",
      "top 5 recommendations:\n",
      "   1. (Programming) [0.999] Why did the developer go broke buying Bitcoin? He kept calling it bytecoin and d...\n",
      "   2. (Programming) [0.999] What goes after USA? USB....\n",
      "   3. (Programming) [0.999] What is the most used language in programming? Profanity....\n",
      "   4. (Programming) [0.999] Why did the developer go to therapy? They had too many unresolved issues....\n",
      "   5. (Programming) [0.999] What do you get when you cross a React developer with a mathematician? A functio...\n",
      "5/5 recommendations from same category\n",
      "source (Dad Joke):\n",
      "   I cut my finger cutting cheese. I know it may be a cheesy story but I feel grate now....\n",
      "top 5 recommendations:\n",
      "   1. (Misc) [1.000] My parents raised me as an only child, which really annoyed my younger brother....\n",
      "   2. (Dad Joke) [0.999] What is worse then finding a worm in your Apple? Finding half a worm in your App...\n",
      "   3. (Dad Joke) [0.999] I saw an ad in a shop window, \"Television for sale, $1, volume stuck on full\", I...\n",
      "   4. (Dad Joke) [0.999] What do computers and air conditioners have in common? They both become useless ...\n",
      "   5. (Dad Joke) [0.999] Did you hear about the submarine industry? It really took a dive......\n",
      "4/5 recommendations from same category\n",
      "source (Misc):\n",
      "   What is the leading cause of dry skin? Towels...\n",
      "top 5 recommendations:\n",
      "   1. (Dad Joke) [1.000] How do you fix a broken pizza? With tomato paste....\n",
      "   2. (Misc) [1.000] How do you make a tissue dance? You put a little boogie on it....\n",
      "   3. (Misc) [1.000] Why is peter pan always flying? Because he neverlands...\n",
      "   4. (Misc) [1.000] Never date a baker. They're too kneady....\n",
      "   5. (Misc) [0.999] What kind of bagel can fly? A plain bagel....\n",
      "4/5 recommendations from same category\n"
     ]
    }
   ],
   "source": [
    "test_recommendation_quality(model, data, balanced_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "joke_recommender_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
